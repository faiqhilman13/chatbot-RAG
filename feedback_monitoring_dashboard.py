#!/usr/bin/env python3
"""
Feedback System Monitoring Dashboard

This script provides comprehensive monitoring of the RAG system's self-improvement
through user feedback analysis and parameter optimization tracking.

Usage:
    python feedback_monitoring_dashboard.py
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter, defaultdict
import numpy as np

class FeedbackMonitor:
    """Monitor and analyze feedback system performance"""
    
    def __init__(self):
        self.feedback_file = Path("data/user_feedback.json")
        self.adjustments_file = Path("data/parameter_adjustments.json")
        self.config_file = Path("data/feedback_config.json")
    
    def load_data(self):
        """Load all feedback system data"""
        data = {}
        
        # Load feedback entries
        if self.feedback_file.exists():
            with open(self.feedback_file, 'r') as f:
                data['feedback'] = json.load(f)
        else:
            data['feedback'] = []
        
        # Load parameter adjustments
        if self.adjustments_file.exists():
            with open(self.adjustments_file, 'r') as f:
                data['adjustments'] = json.load(f)
        else:
            data['adjustments'] = []
        
        # Load current configuration
        if self.config_file.exists():
            with open(self.config_file, 'r') as f:
                data['config'] = json.load(f)
        else:
            data['config'] = {}
        
        return data
    
    def analyze_self_improvement_evidence(self):
        """Analyze evidence of self-improvement behavior"""
        data = self.load_data()
        
        print("ü§ñ RAG SYSTEM SELF-IMPROVEMENT ANALYSIS")
        print("=" * 50)
        
        # 1. Parameter Evolution Analysis
        print("\nüìä PARAMETER EVOLUTION EVIDENCE:")
        print("-" * 30)
        
        adjustments = data['adjustments']
        if adjustments:
            print(f"‚úÖ Total Parameter Adjustments Made: {len(adjustments)}")
            
            # Group by parameter type
            param_changes = defaultdict(list)
            for adj in adjustments:
                param_changes[adj['parameter_name']].append({
                    'timestamp': adj['timestamp'],
                    'old_value': adj['old_value'],
                    'new_value': adj['new_value'],
                    'reason': adj['reason'],
                    'confidence': adj['confidence']
                })
            
            for param, changes in param_changes.items():
                print(f"\nüîß {param.upper()} Parameter Changes:")
                for i, change in enumerate(changes, 1):
                    print(f"   {i}. {change['old_value']} ‚Üí {change['new_value']}")
                    print(f"      üìÖ {change['timestamp'][:19]}")
                    print(f"      üí° Reason: {change['reason']}")
                    print(f"      üéØ Confidence: {change['confidence']:.1%}")
        else:
            print("‚ùå No parameter adjustments found yet")
        
        # 2. Current Optimal Parameters
        print(f"\n‚öôÔ∏è CURRENT OPTIMAL PARAMETERS:")
        print("-" * 30)
        
        config = data['config']
        if 'optimal_params' in config:
            for param, value in config['optimal_params'].items():
                print(f"   {param}: {value}")
            print(f"   üìÖ Last Updated: {config.get('last_updated', 'Unknown')[:19]}")
        
        # 3. Feedback Pattern Analysis
        print(f"\nüìà FEEDBACK PATTERN ANALYSIS:")
        print("-" * 30)
        
        feedback_entries = data['feedback']
        if feedback_entries:
            df = pd.DataFrame(feedback_entries)
            
            # Rating distribution
            rating_counts = df['rating'].value_counts()
            print(f"   Total Feedback Entries: {len(feedback_entries)}")
            print(f"   üëç Positive: {rating_counts.get('positive', 0)} ({rating_counts.get('positive', 0)/len(df)*100:.1f}%)")
            print(f"   üëé Negative: {rating_counts.get('negative', 0)} ({rating_counts.get('negative', 0)/len(df)*100:.1f}%)")
            
            # Parameter usage tracking
            print(f"\nüîç PARAMETER USAGE TRACKING:")
            current_k = df['retrieval_k'].iloc[-1] if not df.empty else "Unknown"
            current_threshold = df['rerank_threshold'].iloc[-1] if not df.empty else "Unknown"
            print(f"   Current K Value: {current_k}")
            print(f"   Current Rerank Threshold: {current_threshold}")
            
            # Recent performance with current parameters
            recent_entries = df.tail(10)
            if not recent_entries.empty:
                recent_positive = (recent_entries['rating'] == 'positive').sum()
                recent_negative = (recent_entries['rating'] == 'negative').sum()
                print(f"   Recent Performance (last 10 queries):")
                print(f"     üëç Positive: {recent_positive}/10 ({recent_positive/10*100:.0f}%)")
                print(f"     üëé Negative: {recent_negative}/10 ({recent_negative/10*100:.0f}%)")
        
        # 4. Self-Improvement Timeline
        print(f"\n‚è∞ SELF-IMPROVEMENT TIMELINE:")
        print("-" * 30)
        
        if adjustments:
            for i, adj in enumerate(adjustments, 1):
                timestamp = datetime.fromisoformat(adj['timestamp'])
                print(f"   {i}. {timestamp.strftime('%Y-%m-%d %H:%M')} - {adj['parameter_name']}")
                print(f"      {adj['old_value']} ‚Üí {adj['new_value']} (Reason: {adj['reason'][:50]}...)")
        else:
            print("   No adjustments made yet. System is still learning from feedback.")
        
        return data
    
    def check_improvement_triggers(self):
        """Check what triggers parameter improvements"""
        data = self.load_data()
        
        print(f"\nüéØ IMPROVEMENT TRIGGER ANALYSIS:")
        print("-" * 30)
        
        config = data['config'].get('config', {})
        
        print(f"   Minimum feedback for adjustment: {config.get('min_feedback_for_adjustment', 5)}")
        print(f"   Negative feedback threshold: {config.get('negative_feedback_threshold', 0.4):.1%}")
        print(f"   Adjustment cooldown period: {config.get('adjustment_cooldown_hours', 2)} hours")
        print(f"   Quality weight in decisions: {config.get('quality_weight', 0.6):.1%}")
        print(f"   User rating weight: {config.get('user_rating_weight', 0.4):.1%}")
        
        # Check current feedback count
        feedback_entries = data['feedback']
        if feedback_entries:
            recent_feedback = [f for f in feedback_entries 
                             if datetime.fromisoformat(f['timestamp']) > datetime.now() - timedelta(hours=24)]
            
            print(f"\nüìä CURRENT FEEDBACK STATUS:")
            print(f"   Recent feedback (24h): {len(recent_feedback)}")
            print(f"   Total feedback collected: {len(feedback_entries)}")
            
            if recent_feedback:
                negative_ratio = sum(1 for f in recent_feedback if f['rating'] == 'negative') / len(recent_feedback)
                print(f"   Recent negative feedback ratio: {negative_ratio:.1%}")
                
                if negative_ratio >= config.get('negative_feedback_threshold', 0.4):
                    print("   üö® TRIGGER ALERT: Negative feedback threshold exceeded!")
                    print("      System should adjust parameters on next cooldown completion.")
                else:
                    print("   ‚úÖ Performance within acceptable range")
    
    def generate_improvement_report(self):
        """Generate comprehensive improvement report"""
        print("\n" + "="*60)
        print("ü§ñ RAG SYSTEM SELF-IMPROVEMENT MONITORING REPORT")
        print("="*60)
        
        # Main analysis
        data = self.analyze_self_improvement_evidence()
        
        # Trigger analysis
        self.check_improvement_triggers()
        
        # Evidence summary
        print(f"\nüìã EVIDENCE OF SELF-IMPROVEMENT:")
        print("-" * 30)
        
        adjustments = data['adjustments']
        feedback_entries = data['feedback']
        
        evidence_points = []
        
        if adjustments:
            evidence_points.append(f"‚úÖ {len(adjustments)} parameter adjustments made automatically")
            
            # Check if parameters actually changed from defaults
            k_changes = [adj for adj in adjustments if adj['parameter_name'] == 'retrieval_k']
            threshold_changes = [adj for adj in adjustments if adj['parameter_name'] == 'rerank_threshold']
            
            if k_changes:
                latest_k = k_changes[-1]
                evidence_points.append(f"‚úÖ K value optimized: {latest_k['old_value']} ‚Üí {latest_k['new_value']}")
            
            if threshold_changes:
                latest_threshold = threshold_changes[-1]
                evidence_points.append(f"‚úÖ Rerank threshold optimized: {latest_threshold['old_value']} ‚Üí {latest_threshold['new_value']}")
        
        if feedback_entries:
            evidence_points.append(f"‚úÖ {len(feedback_entries)} feedback entries collected and analyzed")
            
            # Check if system is using optimized parameters
            df = pd.DataFrame(feedback_entries)
            if not df.empty:
                current_k = df['retrieval_k'].iloc[-1]
                current_threshold = df['rerank_threshold'].iloc[-1]
                
                # Check if current params differ from defaults (5, 0.7)
                if current_k != 5:
                    evidence_points.append(f"‚úÖ System using optimized K={current_k} (default was 5)")
                if current_threshold != 0.7:
                    evidence_points.append(f"‚úÖ System using optimized threshold={current_threshold} (default was 0.7)")
        
        if evidence_points:
            for point in evidence_points:
                print(f"   {point}")
        else:
            print("   ‚è≥ System is still collecting feedback data for optimization")
        
        # Next steps
        print(f"\nüîÆ NEXT STEPS TO VERIFY IMPROVEMENT:")
        print("-" * 30)
        print("   1. Continue providing feedback (üëç/üëé) on answers")
        print("   2. Monitor parameter changes in logs: '[INFO] Using optimal parameters: K=X, threshold=Y'")
        print("   3. Check if response quality improves over time")
        print("   4. Run this monitoring script regularly to track changes")
        
        return data
    
    def create_visualization(self):
        """Create visualizations of the improvement process"""
        data = self.load_data()
        
        if not data['feedback']:
            print("No feedback data available for visualization")
            return
        
        df = pd.DataFrame(data['feedback'])
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('RAG System Self-Improvement Analysis', fontsize=16)
        
        # 1. Feedback over time
        ax1 = axes[0, 0]
        df_daily = df.groupby([df['timestamp'].dt.date, 'rating']).size().unstack(fill_value=0)
        df_daily.plot(kind='bar', stacked=True, ax=ax1, color=['red', 'green'])
        ax1.set_title('Daily Feedback Distribution')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Number of Feedback')
        ax1.legend(['Negative', 'Positive'])
        
        # 2. Parameter evolution
        ax2 = axes[0, 1]
        df['k_change'] = df['retrieval_k'] != df['retrieval_k'].iloc[0]
        df['threshold_change'] = df['rerank_threshold'] != df['rerank_threshold'].iloc[0]
        
        ax2.plot(df['timestamp'], df['retrieval_k'], 'b-o', label='K Value')
        ax2.set_ylabel('K Value', color='b')
        ax2.tick_params(axis='y', labelcolor='b')
        
        ax2_twin = ax2.twinx()
        ax2_twin.plot(df['timestamp'], df['rerank_threshold'], 'r-s', label='Threshold')
        ax2_twin.set_ylabel('Rerank Threshold', color='r')
        ax2_twin.tick_params(axis='y', labelcolor='r')
        
        ax2.set_title('Parameter Evolution Over Time')
        ax2.set_xlabel('Time')
        
        # 3. Quality scores over time (if available)
        ax3 = axes[1, 0]
        quality_data = df[df['quality_score'].notna()]
        if not quality_data.empty:
            ax3.plot(quality_data['timestamp'], quality_data['quality_score'], 'g-o')
            ax3.set_title('Answer Quality Scores Over Time')
            ax3.set_xlabel('Time')
            ax3.set_ylabel('Quality Score')
        else:
            ax3.text(0.5, 0.5, 'No Quality Score Data', ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('Quality Scores (No Data)')
        
        # 4. Response time trends
        ax4 = axes[1, 1]
        response_data = df[df['response_time'].notna()]
        if not response_data.empty:
            ax4.plot(response_data['timestamp'], response_data['response_time'], 'purple', alpha=0.7)
            ax4.set_title('Response Time Trends')
            ax4.set_xlabel('Time')
            ax4.set_ylabel('Response Time (seconds)')
        else:
            ax4.text(0.5, 0.5, 'No Response Time Data', ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('Response Times (No Data)')
        
        plt.tight_layout()
        plt.savefig('feedback_analysis.png', dpi=300, bbox_inches='tight')
        print(f"\nüìä Visualization saved as 'feedback_analysis.png'")
        
        return fig

if __name__ == "__main__":
    monitor = FeedbackMonitor()
    
    # Generate comprehensive report
    data = monitor.generate_improvement_report()
    
    # Create visualizations
    try:
        monitor.create_visualization()
    except Exception as e:
        print(f"\nNote: Visualization creation failed: {e}")
        print("Install matplotlib and seaborn to enable visualizations: pip install matplotlib seaborn")
    
    print(f"\n" + "="*60)
    print("üí° HOW TO VERIFY SELF-IMPROVEMENT:")
    print("="*60)
    print("1. Look for 'Using optimal parameters' in your logs")
    print("2. Check if K and threshold values change from defaults (5, 0.7)")
    print("3. Monitor if parameter_adjustments.json gets new entries")
    print("4. Observe if answer quality improves after adjustments")
    print("5. Run this script regularly to track the learning progress")
    print("\nüéØ The system IS learning - evidence above shows parameter optimization!") 