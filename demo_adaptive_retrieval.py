#!/usr/bin/env python3
"""
Demonstration of Adaptive Retrieval Intelligence and Enhanced Source Attribution

This script demonstrates the new features implemented:
1. Dynamic Retrieval K based on query type and complexity
2. Query Complexity Classification  
3. Adaptive Chunk Size Selection
4. Chunk Anchoring with Source Metadata
5. Source Citation Validation
6. Cross-Document Reference Detection
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.utils.query_analyzer import query_analyzer, QueryType, QueryComplexity
from app.utils.source_attribution import source_attribution_manager
from langchain.schema import Document

def demonstrate_adaptive_retrieval():
    """Demonstrate the adaptive retrieval intelligence features."""
    
    print("ğŸ§  ADAPTIVE RETRIEVAL INTELLIGENCE DEMONSTRATION")
    print("=" * 60)
    
    # Test queries of different types and complexities
    test_queries = [
        "Who is Faiq Hilman?",  # Simple entity query
        "What is machine learning?",  # Definition query
        "Summarize his work experience",  # Summary query
        "Why did he choose to work at PwC and how did this decision impact his career?",  # Complex reasoning
        "Compare and analyze the different roles he had across multiple companies, evaluate their strategic importance, and synthesize insights about his career progression pattern",  # Very complex
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ“ Query {i}: {query}")
        print("-" * 40)
        
        # Analyze the query
        analysis = query_analyzer.analyze_query(query)
        
        print(f"ğŸ¯ Query Type: {analysis.query_type.value.upper()}")
        print(f"ğŸ”¥ Complexity: {analysis.complexity.value.upper()}")
        print(f"ğŸ“Š Optimal K: {analysis.optimal_k} documents")
        print(f"ğŸ“ Recommended Chunk Size: {analysis.chunk_size} tokens")
        print(f"ğŸ”— Recommended Overlap: {analysis.chunk_overlap} tokens")
        print(f"ğŸª Confidence: {analysis.confidence:.2f}")
        
        if analysis.detected_entities:
            print(f"ğŸ‘¤ Entities: {', '.join(analysis.detected_entities)}")
        
        if analysis.keywords:
            print(f"ğŸ” Keywords: {', '.join(analysis.keywords[:5])}")  # Show first 5

def demonstrate_source_attribution():
    """Demonstrate the enhanced source attribution features."""
    
    print("\n\nğŸ§¾ ENHANCED SOURCE ATTRIBUTION DEMONSTRATION")
    print("=" * 60)
    
    # Create sample documents
    sample_docs = [
        Document(
            page_content="Faiq Hilman worked as a Senior Consultant at PricewaterhouseCoopers (PwC) from January 2020 to December 2022. He led data analytics projects for Fortune 500 clients.",
            metadata={
                "source": "faiq_cv.pdf",
                "page": 1,
                "title": "Faiq Hilman CV",
                "doc_id": "cv_001"
            }
        ),
        Document(
            page_content="During his tenure at PwC, Faiq specialized in machine learning implementations and helped clients reduce operational costs by an average of 15%.",
            metadata={
                "source": "faiq_cv.pdf", 
                "page": 2,
                "title": "Faiq Hilman CV",
                "doc_id": "cv_001"
            }
        ),
        Document(
            page_content="Tesla reported record revenue of $96.8 billion in fiscal year 2024, representing a 15% increase from the previous year.",
            metadata={
                "source": "tesla_fy24.pdf",
                "page": 5,
                "title": "Tesla Annual Report 2024",
                "doc_id": "tesla_001"
            }
        )
    ]
    
    print(f"\nğŸ“š Original Documents: {len(sample_docs)}")
    for i, doc in enumerate(sample_docs, 1):
        print(f"   {i}. {doc.metadata['source']} (Page {doc.metadata['page']})")
    
    # Create anchored chunks
    print(f"\nğŸ”— Creating Anchored Chunks...")
    anchored_docs = source_attribution_manager.create_anchored_chunks(sample_docs)
    
    print(f"\nğŸ“‹ Anchored Content Example:")
    print("=" * 40)
    print(anchored_docs[0].page_content[:200] + "...")
    
    # Detect cross-document references
    print(f"\nğŸ” Cross-Document Reference Detection:")
    cross_refs = source_attribution_manager.detect_cross_document_references(anchored_docs)
    
    if cross_refs:
        for ref_pair, themes in cross_refs.items():
            print(f"   ğŸ“ {ref_pair}: {', '.join(themes[:3])}")
    else:
        print("   âœ… No cross-references detected (sources cover different topics)")
    
    # Test citation validation
    print(f"\nğŸ“ Citation Validation Example:")
    sample_answer = "[SOURCE: faiq_cv.pdf | PAGE: 1] Faiq Hilman worked as a Senior Consultant at PwC from 2020 to 2022."
    validation = source_attribution_manager.validate_answer_citations(sample_answer, anchored_docs)
    
    print(f"   âœ… Valid Citations: {len(validation.valid_citations)}")
    print(f"   âŒ Invalid Citations: {len(validation.invalid_citations)}")
    print(f"   âš ï¸  Missing Citations: {len(validation.missing_citations)}")
    print(f"   ğŸ“Š Citation Accuracy: {validation.citation_accuracy:.2f}")
    
    if validation.recommendations:
        print(f"   ğŸ’¡ Recommendations: {'; '.join(validation.recommendations)}")
    
    # Generate source-aware prompt
    print(f"\nğŸ¯ Source-Aware Prompt Generation:")
    question = "What did Faiq do at PwC?"
    prompt = source_attribution_manager.generate_source_aware_prompt(question, anchored_docs)
    
    print("=" * 40)
    print(prompt[:400] + "...")
    print("=" * 40)

def demonstrate_integrated_features():
    """Demonstrate how adaptive retrieval and source attribution work together."""
    
    print("\n\nğŸš€ INTEGRATED ADAPTIVE RETRIEVAL DEMONSTRATION")
    print("=" * 60)
    
    queries = [
        ("Who is Faiq?", "Simple entity query"),
        ("Summarize Faiq's consulting experience", "Summary query"),
        ("Analyze the strategic impact of Faiq's work at PwC", "Complex reasoning query")
    ]
    
    for query, description in queries:
        print(f"\nğŸ¯ {description}: '{query}'")
        print("-" * 50)
        
        # Step 1: Analyze query
        analysis = query_analyzer.analyze_query(query)
        print(f"1ï¸âƒ£  Query Analysis: {analysis.query_type.value} | {analysis.complexity.value} | K={analysis.optimal_k}")
        
        # Step 2: Simulate retrieval (in real system, this would query the vector store)
        print(f"2ï¸âƒ£  Simulated Retrieval: Would retrieve {analysis.optimal_k} most relevant chunks")
        
        # Step 3: Source attribution
        print(f"3ï¸âƒ£  Source Attribution: Would add explicit source anchors to all chunks")
        
        # Step 4: Enhanced prompting
        print(f"4ï¸âƒ£  Enhanced Prompting: Would use source-aware prompt with citation requirements")
        
        # Step 5: Citation validation  
        print(f"5ï¸âƒ£  Citation Validation: Would validate answer citations and provide recommendations")

def main():
    """Main demonstration function."""
    
    print("ğŸª ADAPTIVE RETRIEVAL & SOURCE ATTRIBUTION DEMO")
    print("=" * 70)
    print("\nThis demonstration shows the new RAG features:")
    print("âœ… Dynamic Retrieval K based on query analysis")
    print("âœ… Query complexity classification")
    print("âœ… Adaptive chunk size recommendations")
    print("âœ… Chunk anchoring with source metadata")
    print("âœ… Source citation validation")
    print("âœ… Cross-document reference detection")
    print("âœ… Source-aware prompt generation")
    
    try:
        demonstrate_adaptive_retrieval()
        demonstrate_source_attribution()
        demonstrate_integrated_features()
        
        print("\n\nğŸ‰ DEMONSTRATION COMPLETED SUCCESSFULLY!")
        print("=" * 70)
        print("\nğŸ“– The adaptive retrieval system is now ready to:")
        print("   ğŸ” Automatically optimize retrieval parameters based on query characteristics")
        print("   ğŸ“š Provide enhanced source attribution with explicit citations")
        print("   ğŸ”— Detect and handle cross-document references")
        print("   âœ… Validate citation accuracy in generated answers")
        print("   ğŸ¯ Generate source-aware prompts for better LLM responses")
        
        print("\nğŸš€ These features significantly improve:")
        print("   ğŸ“ˆ Retrieval accuracy and relevance")
        print("   ğŸ”’ Source transparency and traceability")
        print("   ğŸ¯ Answer quality and factual grounding")
        print("   ğŸ›¡ï¸  Reduced hallucination and source mixing")
        
    except Exception as e:
        print(f"\nâŒ Error during demonstration: {str(e)}")
        print("ğŸ’¡ This might be due to missing dependencies (spaCy model)")
        print("   Run: python -m spacy download en_core_web_sm")

if __name__ == "__main__":
    main() 